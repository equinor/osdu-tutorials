# Equinor specific info

Code for:

* *create_missing_reference_id_manifests.py* - Checking for missing reference id's in a file(s) that would otherwise fail in the referential integrity check and generation of corresponding manifest files
* *check_missing_ids.py* - Checking for manifest id's in a file(s) that are not present in OSDUmanifest files
* *bulk_load_manifests.py* - Bulk uploading of manifest files

Based upon code from:
~~~
git clone https://community.opengroup.org/osdu/platform/bulk-loader.git
~~~ 

For all scripts use the -h option for information about usage.

:warning: Note that these scripts were hacked together quickly and may not be complete / might contain bugs. No further cleanup is planned, although any contributions are welcome.

## Setup

### Virtual Environments
When the current working directory is this repo, a virtual environment is created by
~~~
python -m venv venv
~~~
where the first `venv` is the module exectued; and the second `venv` is the directory name.

## Activation in a shell
### Activate for Mac OS / Linux:

~~~
source venv/bin/activate
~~~
### Activate for Windows

~~~
venv\Scripts\activate
~~~

## Dependencies
Install the dependencies:

~~~
pip install -r requirements.txt
~~~

## Modify initialization with Equinor values
Modify config/dataload.ini

## Add authorization info in .envrc
Follow .envrc_template to set .envrc
~~~
source .envrc
~~~ 

# Configuration

## Python environment

 Check if you have Python 3.7 or later installed with 
~~~
python -V
~~~ 
Go to https://www.python.org/ and get the appropriate version for your operating system if you 
don't. 

Install required libraries with 
~~~
pip install -r requirements.txt
~~~
Using a virtual environment is desirable but not mandatory.

## Environment variables

**CLIENT_ID** - user ID acquired from current OpenID provider.

**CLIENT_SECRET** - user secret acquired from current OpenID provider.

**REFRESH_TOKEN** - refresh token acquired from current OpenID provider.

These variables are required for OpenID flow.

## Config files
All config files are located in **config** dir.

## create_schema.ini

Configuration file for **create_schema.py** script. See details below.

### [KINDS_BASE] section

Options:

* **initial_version**, **enriched_version** - versions for initial and enriched schemas to be created in 
Storage (see details in section for **create_schema.py** script in script definitions section below);

* **source**, **partition** - source and partition ids for schemas ids;

* **sleep_time** - delay in seconds after posting schema and before checking its presence in store. 
Can be both integer and float.

Example of **[KINDS_BASE]** section:
~~~
[KINDS_BASE]
initial_version = 3.0.9
enriched_version = 3.0.10
partition = opendes
source = osdu
sleep_time = 2
~~~

## kinds_mapping.ini

The file contains mapping between **kind name** and **kind code** for schema id.

**Kind name** is kind label from **ResourceTypeID** field of manifests (see example in **Annex**) in
lower case.

**Kind code** is kind label from **kind** field of schemas JSON (see example in **Annex**).

**Initial schema id** for each kind will be composed like *{partition}{source}{kind_code}
{initial_version}* and added like *{kind_name}_kind* option in **[KINDS_INITIAL]** section of 
**dataload.ini** file.

**Enriched schema id** for each kind will be composed like *{partition}{source}{kind_code}
{enriched_version}* and added like *enriched_{kind_name}_kind* option in **[KINDS_ENRICHED]** 
section of **dataload.ini** file.

## dataload.ini

Configuration file for **dataload.py** script. 

### [REQUEST] section

Section to configure your OSDU credentials:
* legal tag,
* acl viewer,
* acl owner.

You can get them from your system administrator.

### [CONNECTION] section

Section to setup details of search, storage and auth API usage.

* storage_url - storage API URL.
* search_url - search API URL.
* schemas_url - URL for posting and retrieving schemas definitions.  
* data-partition-id - Value for header 
* token_endpoint - Auth URL used to get new or refresh expired access tokens
* token_type - token type, that script should use for request authorization. Possible values are *id_token* or *access_token*.
* retries - number of retries attempts on connection error or response with code 500.
* timeout - delay time (in seconds) before request retrying
* batch_size - number of items in array, that is sent to storage. 

### [FILE_PATH_REPLACER] section
Section to setup details of `replace_file_path.py` script workflow.
* blobs_base_url - URL prefix for `PreLoadFilePath` field value in manifests.

### [KINDS_INITIAL] and [KINDS_ENRICHED] sections

Initial and enriched schema ids for each kind. These sections are generated by **create_schema.py** 
script (see more details in section for **dataload.py** script in script definitions section below).

## test.ini 

The file contains configuration data for test script.

### [CONNECTION] section

You can set number of **request retries** and **request timeout** in this section.

### [COUNTS] section

In this section you can set the expected number of resources of each kind.
Documents number should be a sum of markers, trajectories, and logs.

### [KINDS_TO_COUNTS_MAPPING] section

This section maps kind entities to the respective option in **[COUNTS]** section. 
This way it is easier to update the latter one because you can map one **[COUNTS]** option to several kinds. 

# Python script definitions 
 
This section contains Python scripts definitions for OSDU platform data loading and testing.

* **create_schema.py** - Python script to add schemas to storage.

    The script uses values from section **[KINDS_BASE]** of **create_schema.ini** to generate full 
    schemas IDs (initial and enriched schema ids for each kind) to put into **[KINDS_INITIAL]** and 
    **[KINDS_ENRICHED]** sections of **dataload.ini** file. Only ids for uploaded schemas are added to dataload.ini. 
    
    **[KINDS_INITIAL]** and **[KINDS_ENRICHED]** sections are used by **dataload.py** and 
    **enrichment.py** scripts later.
    
    The script:
    1.	updates value of "kind" field for each schema from provided directory to ID from 
    [KINDS_INITIAL] or [KINDS_ENRICHED] sections; 
    2.	adds the schema definition to OpenDES Storage. Use correct option name from [KINDS_INITIAL] or 
    [KINDS_ENRICHED] sections to get expected value;
    3.	checks if schema was uploaded successfully.
    
    Recursive folder reading is supported. 

* **dataload.py** - Python script to load manifests to storage.

    Script determines data type automatically according to **ResourceTypeID** in manifest file.
    
    Script:
    1.	is able to load different types of data from one folder and it reads folder recursively.
    2.	Before starting loading process script checks the name of the schemes indicated in the 
    **dataload.ini** config file for their presence on the server. If all schemas exist the 
    script logs all schemas kinds found in the manifests. If even one schema does not exist the
    script will stop with error and prints all non-existed schemas kinds. In this case all missing 
    kinds in config file will be print.
    ~~~
    Error: not all schemas from KINDS_INITIAL section exist in storage.
    Non-existing schemas are:
    opendes:osdu:wellboremarker-wpc:3.0.9
    opendes:osdu:wellboremarker-wp:3.0.9
    opendes:osdu:welllog-wpc:3.0.9
    opendes:osdu:well-master:3.0.9
    opendes:osdu:file:3.0.9
    opendes:osdu:wellboretrajectory-wp:3.0.9
    opendes:osdu:wellbore-master:3.0.9
    opendes:osdu:welllog-wp:3.0.9
    opendes:osdu:wellboretrajectory-wpc:3.0.9
    Data loading process is stopped.
    ~~~
    3. during loading process prints info message about sent schema kinds in current batch request. 
    In the end script prints total count of sent files aggregate by schema kinds.
    ~~~
    All data was loaded. Total counts are:
    Well: 1
    File: 3
    WellboreMarker_wpc: 1
    WellboreMarker_wp: 1
    WellboreTrajectory_wpc: 1
    WellboreTrajectory_wp: 1
    WellLog_wpc: 1
    WellLog_wp: 1
    Wellbore: 1
    ~~~

* **test_uploaded_data_count.py** - Python script to check if the number of uploaded resources in 
storage is correct. Uses **dataload.ini** and **test.ini** configs (see **Configuration** section).
You can test either initial kinds, or enriched kinds, or both.

* **test_files_in_dir.py** - Python script to test if data from files in specified directories was 
uploaded and indexed correctly. 

* **enrichment.py** - Python script for enriching data in storage.

    Script can use data from **execution.log** file that is created on data load step.
    
* **schemas_cleanup.py** - Python script to delete list of kinds.
    
    Update **KINDS_TO_DELETE** constant in the script to choose kinds to delete. 
    
* **data_cleaner.py** - Python script to delete records from storage.
    
* **update_artefacts.py** - Python script for post updating the WPC with the new file srn in the Artefacts section. 
    It update the **Artefacts** and **ArtefactFiles** field.

    Script uses csv files (e.g. **execution.log** created with **dataload.py** script) to delete list of records.

    Script uses data from **execution.log** file that is created on data load step.  
    On the first step the script **try to find** file record with specified parameters to avoid creating duplicate record.
    In case of failure the script will create new one file record.  
    A new **update_artefacts_{timestamp}.log** file is created for each launch. It logs **artefact file resource id**, **updated record ids** and **skipped record ids**.
    ~~~
    created: srn:file/ovds:0714484884019509291210:
    skipped: opendes:doc:ee5d90385d68497bba75f73a3015bb85
    updated: opendes:doc:e60277b4315741e58967354631d91522
    skipped: opendes:doc:ede7d422799c4914ba9897592c114516
    skipped: opendes:doc:3a9b1b1076864f159004ad793da34c99
    updated: opendes:doc:699f8e44f28b4f0299340a7f7f047c25
    skipped: opendes:doc:4c7379ed2cb245b298fcce5625c0c591
    ~~~

* **replace_file_path.py** - Utility script for updating `PreLoadFilePath` field value in **work-product** manifests.

### Notes

It is preferred to use **test_uploaded_data_count.py** script because it is way faster and sufficient 
enough. **test_files_in_dir.py** script is mostly used in the development stage for now.

Please take into consideration that indexing takes some time after upload. 

# Usage

### 1. Download required manifests and schemas JSON-s to your PC

You can find **manifests** in folder **4-instances** in this 
[repository](https://dev.azure.com/slb-des-ext-collaboration/open-data-ecosystem/_git/osdu-test-data?path=%2F&version=GBmaster_osdu_r2_updated&_a=contents).

You can find **schemas** in this 
[repository](https://dev.azure.com/slb-des-ext-collaboration/open-data-ecosystem/_git/opendes-osdu-test-data?version=GBtemplates&_a=contents).

### 2. Update configuration

Check **Configutation** section of this readme.

### 3. Run **create_schema.py** script

~~~
python create_schema.py --dir <schemas_dir>
~~~
where **schemas_dir** is path (absolute or relative) to folder with schemas templates.

Each schema in this directory should use template string for "kind" field value. Template should 
include option name from **[KINDS_INITIAL]** or **[KINDS_ENRICHED]** section. Possible values are 
*{kind_name}\_kind* or *enriched\_{kind_name}\_kind*. 

Schema JSON example:
~~~
{
    ...
    "kind": "{well_kind}", // CORRECT
    ...
}
~~~
You can find some samples in **schemas_templates** directory of 
[osdu-test-data-schemas](https://dev.azure.com/slb-des-ext-collaboration/open-data-ecosystem/_git/opendes-osdu-test-data?version=GBtemplates&_a=contents) repository. 
Script will substitute "kind" field value with actual option value from **[KINDS_INITIAl]** and 
**[KINDS_ENRICHED]** sections before adding the schema to storage.

### OPTIONAL. Run **replace_file_path.py** script
~~~
python replace_file_path.py --manifests_dir <manifests_dir>
~~~
where **manifests_dir** is path (absolute or relative) to folder with **work-product** manifests.


### 4. Run **dataload.py** script  

~~~
python dataload.py --dir <root_dir>
~~~

where **root_dir** - relative path to folder with manifests in relation to script location or 
absolute path.

### 5. Run test scripts

#### Running **test_uploaded_data_count.py** script:

~~~
python test_uploaded_data_count.py -i -e
~~~
* **-h** or **--help** - get information how to use the script
* **-i** or **--initial** - test initial kinds
* **-e** or **--enriched** - test enriched kinds

If no parameters were entered script tests initial kinds.

#### Running **test_files_in_dir.py** script:

~~~
python test_files_in_dir.py --well_dir <well_dir> --wellbore_dir <wellbore_dir> 
--wellboremarker_dir <wellboremarker_dir> --wellboretrajectory_dir <wellboretrajectory_dir> 
--welllog_dir <welllog_dir> --limit <limit>
~~~

**well_dir**, **wellbore_dir**, **wellboremarker_dir**, **wellboretrajectory_dir**, 
**welllog_dir** - directories with corresponding data types.

**limit** - number of files to test.

### 6. Run **enrichment.py** script

~~~
python enrichment.py --record_ids <record_ids>
~~~

**record_ids** - file with the list of uploaded to storage resources ids. You can use 
**execution.log** file created on step 2 for convenience.


## [Optional] 7. Run **data_cleaner.py** script

This script is used to delete incorrect data. You can use logs created on dataload or execution steps.

~~~
python data_cleaner.py --purge_file_path <purge_file_path>
~~~

**purge_file_path** - file with the list of uploaded to storage resources ids to delete.


### [OPTIONAL] 8. Run **update_artefacts.py** script

~~~
python update_artefacts.py  --artefact-path <preload_file_path> --artefact-kind <artefact_file_kind> --record-ids <record_ids> --kind-to-update <kind> 
~~~

**preload_file_path** - value for PreLoadFilePath field in artefact file record;  
**artefact_file_kind** - value for kind field in artefact file record. By default `opendes:osdu:file:0.2.0` is used;   
**record_ids** - file with the list of uploaded to storage resources ids You can use 
**execution.log** file created on step 2 for convenience;  
**kind** - determines what types of resources should be updated. If the record is the specified kind it (record) will be updated, 
otherwise it will be skipped.


# Incremental upload

### Introduction
Incremental data upload means a specific order of data loading:
1. Primary data upload
2. Enrichment process
3. Seismic data upload
4. Documents upload
5. Reference data upload

### Primary data upload
Primary data types are wells, wellbores, markers, trajectories and well logs. 
At first, schemas has to be created in storage. Use **create_schema.py** for that purpose.
Upload them with **dataload.py**.

You can check if upload was successful with **test/r2_integration_tests_tno.postman_collection.json**. 
Keep in mind, that test cases marked with [X] and [0.2.1] are going to fail at this point.

### Enrichment process
To start enrichment of primary data use **enrichment.py** with **execution.log** file generated on previous step.

Now you can run **test/r2_integration_tests_tno.postman_collection.json** and all of the tests except the ones marked 
with [X] has to pass.

### Seismic data upload
Upload seismic data with **dataload.py**.

To check if the upload has finished successfully you can run **r2_integration_tests_volve.postman_collection.json**.

### Documents upload
Upload documents with **dataload.py**.

### Reference data upload
Upload reference data with **dataload.py**.

# Adding support for new kinds
1. Prepare or get search schema and manifest example. Check **Annex** to see expected JSON structures.
2. Get kind name from value of ***kind*** field of schema.

    Example of **kind** value in schema:
    ~~~
    {
        "kind": "opendes:opendes:seismic_interpretation_project:1.0.0"
        ...
    }
    ~~~
    Kind name in this sample is **seismic_interpretation_project**.

3. Get kind name from value of **ResourceTypeID** field of manifest.
    Check manifest structure samples in **Annex** to understand what field exactly you need.
    Example of **ResourceTypeID** value in manifest:
    ~~~
    {
        "Manifest": {
            "ResourceTypeID": "srn:type:master-data/Wellbore:", 
            ...
        }
    }
    ~~~
    Kind name in this sample is **Wellbore**.
4. Update **kinds_mapping.ini** config with option(s) for new kind.
 
    **Option name** - kind name from **manifest** in **lower case**. If kind is from **work-products** group,
     add two options with **_wp** (for work product) or **_wpc** (for work product component) suffixes.
    
    Examples:
    
    `welloperatingenvironment` - Option name for `WellOperatingEnvironment` kind.
    
    `wellboremarker_wp` - Option name for **work product** of `WellboreMarker` kind.
    
    `wellboremarker_wpc` - Option name for **work product component** of `WellboreMarker` kind.

    **Option value** - kind name from **schema**. Try use dashes instead of underscores.
    Examples: `well-operating-environment`, `well-interest-type` `well-datum-type`.
 5. Substitute **kind** field value in schema with template in next format: `"{variable}"`. Where variable is 
    option name from kinds_mapping.ini + **"\_kind"** suffix. If schema is for enriched version of kind,
    add **"enriched\_"** prefix.      
    Examples:
    ~~~
    {
        "kind": "{wellboremarker_wp_kind}",
        ...
    },
    ~~~
    ~~~
    {
        "kind": "{enriched_wellboremarker_wp_kind}",
        ...
    }
    ~~~
    The `create_schema.py` script will replace "kind" field value with full schema id according to
     `create_schema.ini` config before adding the schema to storage.
     It will add required options in `dataload.ini` for dataload.py script.
     Check **Python script definitions** section for more information.
    

# Annex

Schema example:

~~~
{
  "kind": "{well_kind}", // create_schema.py updates this field value
  "schema": [
    {
      "path": "ResourceTypeID",
      "kind": "string",
      "ext": {}
    },
    ...
  ]
}
~~~

Manifest for master data example:
~~~
{
    "ResourceTypeID": "srn:type:master-data/Well:",
    "Manifest": {
        "ResourceID": "srn:master-data/Well:1000:",
        "ResourceTypeID": "srn:type:master-data/Well:",  // dataload.py uses this field value 
        "Data": {
            "IndividualTypeProperties": {
                ...
            },
            "GroupTypeProperties": {
                ...
            },
            "ExtensionProperties": {
                ...    
            }
        }
        ...
    }
}
~~~

Manifest for work products example:
~~~
{
    "WorkProduct": {
        "ResourceTypeID": "srn:type:work-product/WorkProduct:",
        "Data": {
            "GroupTypeProperties": {
                ...
            },
            "IndividualTypeProperties": {
                ...
            },
            "ExtensionProperties": {}
        },
        ...
    },
    "WorkProductComponents": [
        {
            "ResourceTypeID": "srn:type:work-product-component/WellboreTrajectory:",  // dataload.py uses this field value
            "Data": {
                "GroupTypeProperties": {
                    ...
                },
                "IndividualTypeProperties": {
                    ...
                },
                "ExtensionProperties": {}
            },
            ...
        }
    ],
    "Files": [
        {
            "ResourceTypeID": "srn:type:file/csv:",
            "Data": {
                "GroupTypeProperties": {
                    ...
                },
                "IndividualTypeProperties": {},
                "ExtensionProperties": {}
            },
            ...
        }
    ]
}
~~~